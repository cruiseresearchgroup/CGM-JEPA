import torch
import json
import os
import random
import numpy as np
import math
import wandb

from scipy.interpolate import make_smoothing_spline
from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score, roc_curve, mean_squared_error, mean_absolute_error, r2_score, average_precision_score


# TODO: Call this before pretraining and evaluation
def seed_everything(seed):
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

def init_weights(m):
    if isinstance(m, torch.nn.Linear):
        trunc_normal_(m.weight, std=0.02)
        if m.bias is not None:
            torch.nn.init.constant_(m.bias, 0)
    elif isinstance(m, torch.nn.LayerNorm):
        torch.nn.init.constant_(m.bias, 0)
        torch.nn.init.constant_(m.weight, 1.0)
    
def load_device():
    # load device
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"Using GPU: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("Using MPS (Apple Silicon)")
    else:
        device = torch.device("cpu")
        print("Using CPU")
    # Note: nn.Embedding has a bug with mps, but we're not using nn.Embedding directly
    # If you encounter issues with MPS, you can force CPU here
    return device

def apply_smoothing(lam, x_raw):
    x_arr = np.array(x_raw)
    time_indices = np.arange(len(x_raw))

    # filter any missing values to fit spline
    valid_mask = x_arr != -1
    valid_x = x_arr[valid_mask]
    valid_indices = time_indices[valid_mask]
    spline = make_smoothing_spline(x=valid_indices, y=valid_x, lam=lam)

    # use it to smooth all the timepoints
    x_arr = spline(time_indices)

    return x_arr.tolist()

def get_metrics(predictions, targets, probs=None):
    
    predictions = np.array(predictions)
    targets = np.array(targets)
    
    if probs is None:
        # regression metrics
        mse = mean_squared_error(targets, predictions)
        mae = mean_absolute_error(targets, predictions)
        r2 = r2_score(targets, predictions)
        
        return {
            "mse": mse,
            "mae": mae,
            "r2": r2
        }

    probs = np.array(probs)

    # calculate metrics
    accuracy = 100.0 * np.sum(predictions == targets) / len(targets) if len(targets) > 0 else 0.0

    try:
        auroc = roc_auc_score(targets, probs[:, 1])
        prauc = average_precision_score(targets, probs[:, 1])
        fpr, tpr, thresholds = roc_curve(targets, probs[:, 1])
    except ValueError:
        auroc = 0.0
        prauc = 0.0
        fpr, tpr, thresholds = [], [], []
    
    recall = recall_score(targets, predictions)
    precision = precision_score(targets, predictions)
    f1 = f1_score(targets, predictions)

    return {
        "accuracy": accuracy,
        "auroc": auroc,
        "prauc": prauc,
        "recall": recall,
        "precision": precision,
        "f1_score": f1,
        "roc_curve": {
            "fpr": fpr.tolist(),
            "tpr": tpr.tolist(),
            "thresholds": thresholds.tolist()
        }
    }

def analyze_results(file_path, use_wandb=False, wandb_project=None, experiment_config=None):
    """
    Analyze results from a JSON file and optionally log to wandb.
    
    Args:
        file_path: Path to results JSON file
        use_wandb: Whether to log results to wandb
        wandb_project: wandb project name (required if use_wandb=True)
        experiment_config: Full experiment config dict for reproducibility
    """
    with open(file_path, 'r') as f:
        data = json.load(f)

    task = data.get("task", "classification")
    pipeline = data.get("pipeline", "unknown")
    pipeline_logs = data.get("pipeline_logs", {})
    num_iterations = data.get("num_iterations", 1)

    # Define metric directions
    # min: lower is better
    # max: higher is better
    metric_directions = {
        "mse": "min",
        "mae": "min",
        "r2": "max",
        "accuracy": "max",
        "f1_score": "max",
        "auroc": "max",
        "recall": "max",
        "precision": "max"
    }

    print(f"Task: {task}")
    print(f"Pipeline: {pipeline}")
    print(f"Iterations: {num_iterations}")
    print("-" * 50)

    # Aggregate metrics across iterations and folds
    model_stats = {} # {model_name: {metric: [values]}}

    # Check if pipeline_logs uses nested structure (iteration -> fold) or flat structure (fold only)
    # Nested: keys are integers (iteration numbers)
    # Flat: keys are strings like "fold_0" or integers representing fold numbers
    first_key = next(iter(pipeline_logs.keys())) if pipeline_logs else None
    
    if first_key is not None and isinstance(pipeline_logs[first_key], dict):
        # New nested structure: iteration -> fold -> models
        for iteration, folds in pipeline_logs.items():
            for fold, models in folds.items():
                for model in models:
                    name = model.get("name")
                    
                    if name not in model_stats:
                        model_stats[name] = {}
                    
                    test_metrics = model.get("test", None)
                    val_metrics = model.get("val", None)

                    # Prioritize validation metrics if available (for model selection context)
                    # Otherwise use test metrics
                    used_metrics = val_metrics if len(val_metrics) != 0 else test_metrics

                    for metric, value in used_metrics.items():
                        if metric not in model_stats[name]:
                            model_stats[name][metric] = []
                        model_stats[name][metric].append(value)
    else:
        # Old flat structure: fold -> models (backward compatibility)
        for fold, models in pipeline_logs.items():
            for model in models:
                name = model.get("name")
                
                if name not in model_stats:
                    model_stats[name] = {}
                
                test_metrics = model.get("test", None)
                val_metrics = model.get("val", None)

                # Prioritize validation metrics if available (for model selection context)
                # Otherwise use test metrics
                used_metrics = val_metrics if len(val_metrics) != 0 else test_metrics

                for metric, value in used_metrics.items():
                    if metric not in model_stats[name]:
                        model_stats[name][metric] = []
                    model_stats[name][metric].append(value)

    # Identify all metrics present
    all_metrics = set()
    for m_data in model_stats.values():
        all_metrics.update(m_data.keys())

    all_metrics = sorted(list(all_metrics))    

    # Display results sorted by mean performance
    for metric in all_metrics:
        direction = metric_directions.get(metric, "max")
        
        if metric == "roc_curve":
            continue
        
        print(f"\nMetric: {metric.upper()} ({direction})")
        print("=" * 30)
        
        results = []
        for name, metrics in model_stats.items():
            if metric in metrics:
                if metric == "roc_curve":
                    continue
                values = metrics[metric]
                mean = np.mean(values)
                std = np.std(values)
                results.append((name, mean, std))
        
        # Sort based on direction
        if direction == "min":
            results.sort(key=lambda x: x[1]) # Ascending
        else:
            results.sort(key=lambda x: x[1], reverse=True) # Descending
            
        for name, mean, std in results:
            print(f"{name}: {mean:.4f} +/- {std:.4f}")
    
    # Log to wandb if enabled
    if use_wandb and wandb_project:
        # Filter config to remove non-serializable items (like model objects)
        serializable_config = {}
        run_name = None
        if experiment_config:
            for k, v in experiment_config.items():
                # Skip non-serializable types
                if isinstance(v, (str, int, float, bool, list, dict, type(None))):
                    serializable_config[k] = v
            # Optional: use wandb_run_name from config if provided
            run_name = experiment_config.get("wandb_run_name")
        
        with wandb.init(
            project=f"cgm-{task}",
            name=run_name,
            notes="CGM classification comparison between baseline and encoder based",
            tags=["full comparison", "gluformer", "cgm jepa"],
            config=serializable_config
        ) as run:
            # create aggregated table (mean/std across folds)
            expanded_metrics, all_results, roc_curve_results = create_table_for_wandb(all_metrics, model_stats)
            result_table = wandb.Table(columns=expanded_metrics, data=all_results)
            run.log({"comparison": result_table})
            run.log({"roc_curve_results": roc_curve_results})
            
            # Log fold-level metrics as raw data for statistical testing
            fold_metrics = extract_fold_metrics_for_wandb(pipeline_logs, all_metrics)
            if fold_metrics:
                run.summary["fold_level_metrics"] = fold_metrics
            
            run.use_artifact(f'hadamuhammad-unsw/cgm-jepa/cgm-jepa:{experiment_config["cgm_jepa_version"]}', type="model")
            run.use_artifact(f'hadamuhammad-unsw/gluformer/gluformer:{experiment_config["gluformer_version"]}', type="model")
            run.use_artifact(f'hadamuhammad-unsw/cgm-jepa-glucodensity-separate/cgm-jepa-glucodensity-separate:{experiment_config["cgm_jepa_glu_version"]}', type="model")

def create_table_for_wandb(all_metrics: list, model_stats: dict) -> list:
    # expand all_metrics to include metric_mean and metric_std
    expanded_metric = ["model"]
    for metric in all_metrics:
        if metric == "roc_curve":
            continue
        expanded_metric.append(f"{metric}_mean")
        expanded_metric.append(f"{metric}_std")

    all_results = []
    roc_curve_results = []

    roc_res = dict()

    for model_name, metrics in model_stats.items():
        model_result = [model_name]
        for metric in all_metrics:
            # create the list [model, *all_metrics]
            values = metrics[metric]

            if metric == "roc_curve":
                roc_res = {
                    "model_name": model_name,
                    "roc_curve": metric
                }
            else:
                mean = float(np.mean(values))
                std = float(np.std(values))
                model_result += [mean, std]
                
        all_results.append(model_result)
        
        if not roc_res:
            roc_curve_results.append(roc_res)
    
    return (expanded_metric, all_results, roc_curve_results)

def extract_fold_metrics_for_wandb(pipeline_logs: dict, all_metrics: list):
    """Extract fold-level metrics in a structured format for statistical testing.
    
    Returns a dict with nested structure:
    {
        "model_name": {
            "metric_name": {
                "iteration_0": {
                    "fold_0": value,
                    "fold_1": value,
                    ...
                },
                "iteration_1": {
                    "fold_0": value,
                    "fold_1": value,
                    ...
                }
            },
            ...
        },
        ...
    }
    
    This format preserves the structure and is easy to retrieve via wandb API.
    """
    if not pipeline_logs:
        return None
    
    # Structure: {model_name: {metric: {iteration: {fold: value}}}}
    fold_data = {}
    
    # Check if pipeline_logs uses nested structure (iteration -> fold) or flat structure (fold only)
    first_key = next(iter(pipeline_logs.keys())) if pipeline_logs else None
    
    if first_key is not None and isinstance(pipeline_logs[first_key], dict):
        # New nested structure: iteration -> fold -> models
        # Sort iterations and folds to ensure consistent ordering
        sorted_iterations = sorted(pipeline_logs.keys())
        
        for iteration in sorted_iterations:
            folds = pipeline_logs[iteration]
            sorted_folds = sorted(folds.keys())
            
            for fold in sorted_folds:
                models = folds[fold]
                for model in models:
                    name = model.get("name")
                    test_metrics = model.get("test", {})
                    val_metrics = model.get("val", {})
                    # Prioritize validation metrics if available; else use test
                    used_metrics = val_metrics if val_metrics else test_metrics
                    
                    if name not in fold_data:
                        fold_data[name] = {}
                    
                    for metric in all_metrics:
                        if metric == "roc_curve":
                            continue
                        
                        val = used_metrics.get(metric)
                        if val is not None and not isinstance(val, (list, dict)):
                            # Initialize nested structure
                            if metric not in fold_data[name]:
                                fold_data[name][metric] = {}
                            
                            iter_key = f"iteration_{iteration}"
                            if iter_key not in fold_data[name][metric]:
                                fold_data[name][metric][iter_key] = {}
                            
                            fold_key = f"fold_{fold}"
                            fold_data[name][metric][iter_key][fold_key] = float(val)
    
    return fold_data

def save_model(name, model, path_save):
    """
    Save a model checkpoint.

    Args:
        name: Logical name of the model (for logging/consistency only).
        model: torch.nn.Module or compatible object exposing state_dict().
        path_save: Full file path to write (e.g. ".../Output/x_cgm_jepa.pt").
    """
    save_dict = {"encoder": model.state_dict()}
    try:
        os.makedirs(os.path.dirname(path_save), exist_ok=True)
        torch.save(save_dict, path_save)
    except Exception as e:
        raise RuntimeError("Problem saving checkpoint") from e

def convert_numpy_types(obj):
    """
    Recursively convert numpy types to native Python types for JSON serialization.
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_numpy_types(item) for item in obj)
    return obj